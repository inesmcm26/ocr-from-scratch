{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def parse_annot(src_path):\n",
    "  \"\"\"\n",
    "    Input: path to the annotation file\n",
    "    Output: list of dictionaries containing the bounding box coordinates and the label for each word in image\n",
    "  \"\"\"\n",
    "  annot = [] # list of dictionaries {'poly': , 'text' : }\n",
    "  reader = open(src_path, 'r').readlines()\n",
    "  # read a line\n",
    "  for line in reader:\n",
    "\n",
    "    word = {} # one dict per bounding box\n",
    "    parts = line.strip().split(',') # '641', '173', '656', '168', '657', '181', '643', '187', '###']\n",
    "    label = parts[-1]\n",
    "\n",
    "    # edge case\n",
    "    if label == '1':\n",
    "        label = '###'\n",
    "\n",
    "    line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in parts]\n",
    "    # extract the polygon coordinates: 2D array with x and y coordinates\n",
    "    poly = np.array(list(map(float, line[:8]))).reshape((-1, 2)).tolist() # [[641.0, 173.0], [656.0, 168.0], [657.0, 181.0], [643.0, 187.0]]\n",
    "    if len(poly) < 3:\n",
    "        continue\n",
    "\n",
    "    word['poly'] = poly\n",
    "    word['text'] = label\n",
    "    annot.append(word)\n",
    "\n",
    "  return annot\n",
    "\n",
    "def preprocess_img_annot(image, annots, training = True):\n",
    "\n",
    "  if training:\n",
    "    transform_aug = transform_aug.to_deterministic()\n",
    "    image, annots = transform(transform_aug, image, annots)\n",
    "    image, annots = crop(image, annots)\n",
    "\n",
    "  image, annots = resize(self.img_size, image, annots)\n",
    "  \n",
    "  annots = [ann for ann in annots if Polygon(ann['poly']).is_valid]\n",
    "\n",
    "  return image, annots\n",
    "\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "  \"\"\"\n",
    "  Class that generates batches of data\n",
    "  Returns a batch of images and corresponding YOLO output label matrices\n",
    "  \"\"\"\n",
    "  \n",
    "  # list_IDs = ['img1.jpg', 'img2.jpg', etc]\n",
    "  # label_IDs = ['img1.txt', 'img2.txt', etc]\n",
    "  \n",
    "  def __init__(self, in_folder, label_folder, list_IDs, label_IDs, batch_size = 16,\n",
    "               img_size = 640, min_text_size = 8, shrink_ratio = 0.4, thresh_min = 0.3, thresh_max = 0.7, training = True):\n",
    "      self.in_folder = in_folder\n",
    "      self.label_folder = label_folder\n",
    "      self.list_IDs = list_IDs\n",
    "      self.label_IDs = label_IDs\n",
    "      self.batch_size = batch_size\n",
    "      self.img_size = img_size\n",
    "      self.min_text_size = min_text_size\n",
    "      self.shrink_ratio = shrink_ratio\n",
    "      self.thresh_min = thresh_min\n",
    "      self.thresh_max = thresh_max\n",
    "      self.training = training\n",
    "      # self.on_epoch_end()\n",
    "\n",
    "  def on_epoch_end(self):\n",
    "    \"\"\"\n",
    "    Updates indexes after each ephoc\n",
    "    \"\"\"\n",
    "    self.indexes = np.arange(len(self.list_IDs))\n",
    "\n",
    "  def __len__(self):\n",
    "    return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "  def generate_annotations(self, anns):\n",
    "    # initialize things\n",
    "    gt = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n",
    "    mask = np.ones((self.img_size, self.img_size), dtype=np.float32)\n",
    "    thresh_map = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n",
    "    thresh_mask = np.zeros((self.img_size, self.img_size), dtype=np.float32)\n",
    "\n",
    "    # for each annotation in image\n",
    "    for ann in anns:\n",
    "      # get annotation polygon\n",
    "      poly = np.array(ann['poly']) # [[641.0, 173.0], [656.0, 168.0], [657.0, 181.0], [643.0, 187.0]]\n",
    "      # height and width\n",
    "      height = max(poly[:, 1]) - min(poly[:, 1])\n",
    "      width = max(poly[:, 0]) - min(poly[:, 0])\n",
    "      # polygon\n",
    "      polygon = Polygon(poly)\n",
    "      # generate gt and mask\n",
    "      if polygon.area < 1 or min(height, width) < self.min_text_size or ann['text'] == '###':\n",
    "        cv2.fillPoly(mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
    "        continue\n",
    "      else:\n",
    "        distance = polygon.area * (1 - np.power(self.shrink_ratio, 2)) / polygon.length\n",
    "        subject = [tuple(l) for l in ann['poly']]\n",
    "        padding = pyclipper.PyclipperOffset()\n",
    "        padding.AddPath(subject, pyclipper.JT_ROUND, pyclipper.ET_CLOSEDPOLYGON)\n",
    "        shrinked = padding.Execute(-distance)\n",
    "        if len(shrinked) == 0:\n",
    "          cv2.fillPoly(mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
    "          continue\n",
    "        else:\n",
    "          shrinked = np.array(shrinked[0]).reshape(-1, 2)\n",
    "          if shrinked.shape[0] > 2 and Polygon(shrinked).is_valid:\n",
    "            cv2.fillPoly(gt, [shrinked.astype(np.int32)], 1)\n",
    "          else:\n",
    "            cv2.fillPoly(mask, poly.astype(np.int32)[np.newaxis, :, :], 0)\n",
    "            continue\n",
    "      # generate thresh map and thresh mask\n",
    "      ann['poly'], thresh_map, thresh_mask = draw_thresh_map(ann['poly'], thresh_map, thresh_mask, shrink_ratio = self.shrink_ratio)\n",
    "\n",
    "    thresh_map = thresh_map * (self.thresh_max - self.thresh_min) + self.thresh_min\n",
    "\n",
    "    return gt, mask, thresh_map, thresh_mask\n",
    "\n",
    "  def __data_generation(self, batch_x, batch_y):\n",
    "    \"\"\"\n",
    "    Generates data containing batch_size samples\n",
    "    This code is multi-core friendly\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    masks = []\n",
    "    thresh_maps = []\n",
    "    thresh_masks = []\n",
    "    batch_loss = np.zeros([len(batch_x), ], dtype=np.float32)\n",
    "    \n",
    "    # all annotations from that paths\n",
    "    # all_anns = load_all_anns(gt_paths)\n",
    "\n",
    "\n",
    "    # batch_images = np.zeros([batch_size, image_size, image_size, 3], dtype=np.float32) -> images\n",
    "    # batch_gts = np.zeros([batch_size, image_size, image_size], dtype=np.float32) -> ?\n",
    "    # batch_masks = np.zeros([batch_size, image_size, image_size], dtype=np.float32) -> ?\n",
    "    # batch_thresh_maps = np.zeros([batch_size, image_size, image_size], dtype=np.float32) -> ?\n",
    "    # batch_thresh_masks = np.zeros([batch_size, image_size, image_size], dtype=np.float32) -> ?\n",
    "    # batch_loss = np.zeros([batch_size, ], dtype=np.float32)\n",
    "\n",
    "    for i in range(0, len(batch_x)):\n",
    "      # image path and label path\n",
    "      img_path = self.in_folder + '/' + batch_x[i] # '/content/drive/My Drive/Colab Notebooks/ICDAR2015/Challenge4/ch4_training_images/img_1.jpg'\n",
    "      annot_path = self.label_folder + '/' + batch_y[i] # '/content/drive/My Drive/Colab Notebooks/ICDAR2015/Challenge4/ch4_training_localization_transcription_gt/gt_img_1.txt'\n",
    "\n",
    "      # read image image\n",
    "      image = cv2.imread(img_path)\n",
    "      # # specific image annotations\n",
    "      anns = parse_annot(annot_path) # get image annotations [{'poly', 'text'}, {'poly', 'text'}, etc.}]\n",
    "      \n",
    "      # data augmentation if training + image resizing\n",
    "      image, anns = preprocess_img_annot(image, anns, self.training)\n",
    "\n",
    "      # get different types of annotations\n",
    "      gt, mask, thresh_map, thresh_mask = self.generate_annotations(anns)\n",
    "\n",
    "      # image scaling\n",
    "      image = image.astype(np.float32)\n",
    "      image[..., 0] -= mean[0]\n",
    "      image[..., 1] -= mean[1]\n",
    "      image[..., 2] -= mean[2]\n",
    "\n",
    "      images.append(image)\n",
    "      bboxes.append(gt)\n",
    "      masks.append(mask)\n",
    "      thresh_maps.append(thresh_map)\n",
    "      thresh_masks.append(thresh_mask)\n",
    "\n",
    "    inputs = [np.array(images), np.array(bboxes), np.array(masks), np.array(thresh_maps), np.array(thresh_masks)]\n",
    "    outputs = batch_loss\n",
    "    \n",
    "    return inputs, outputs\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    \"\"\"\n",
    "    Generates one batch of data\n",
    "    \"\"\"\n",
    "\n",
    "    # Gets images indexes for this batch\n",
    "    # indexes = self.indexes[index * self.batch_size : (index + 1)* self.batch_size]\n",
    "\n",
    "    batch_x = self.list_IDs[index * self.batch_size : (index+1) * self.batch_size]\n",
    "    batch_y = self.label_IDs[index * self.batch_size : (index+1) * self.batch_size]\n",
    "\n",
    "    X, y = self.__data_generation(batch_x, batch_y)\n",
    "\n",
    "    return X, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
